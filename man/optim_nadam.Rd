% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/nadam.R
\name{optim_nadam}
\alias{optim_nadam}
\title{Nadam optimizer}
\usage{
optim_nadam(
  params,
  lr = 0.002,
  betas = c(0.9, 0.999),
  eps = 1e-08,
  weight_decay = 0,
  momentum_decay = 0.004
)
}
\arguments{
\item{params}{List of parameters to optimize.}

\item{lr}{Learning rate (default: 1e-3)}

\item{betas}{Coefficients computing running averages of gradient
and its square (default: (0.9, 0.999)).}

\item{eps}{Term added to the denominator to improve numerical
stability (default: 1e-8).}

\item{weight_decay}{Weight decay (L2 penalty) (default: 0).}

\item{momentum_decay}{Momentum_decay (default: 4e-3).}
}
\value{
A torch optimizer object implementing the \code{step} method.
}
\description{
R implementation of the Nadam optimizer proposed
by Dazat (2016).

From the abstract by the paper by Dazat (2019):
This work aims to improve upon the recently proposed and
rapidly popularized optimization algorithm Adam (Kingma & Ba, 2014).
Adam has two main components—a momentum component and an adaptive
learning rate component. However, regular momentum can be shown conceptually
and empirically to be inferior to a similar algorithm known as
Nesterov’s accelerated gradient (NAG).
}
\references{
Timothy Dozat,
"Incorporating Nesterov Momentum into Adam",
International Conference on Learning Representations (ICLR) 2019.
https://openreview.net/pdf/OM0jvwB8jIp57ZJjtNEZ.pdf
}
\author{
Gilberto Camara, \email{gilberto.camara@inpe.br}

Rolf Simoes, \email{rolf.simoes@inpe.br}

Felipe Souza, \email{lipecaso@gmail.com}

Alber Sanchez, \email{alber.ipia@inpe.br}
}
