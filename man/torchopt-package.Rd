% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/torchopt-package.R
\docType{package}
\name{torchopt-package}
\alias{torchopt}
\alias{torchopt-package}
\title{torchopt: Advanced Optimizers for Torch}
\description{
Optimizers for 'torch' deep learning library. These functions include recent results published in the literature and are not part of the optimizers offered in 'torch'. Prospective users should test these optimizers with their data, since performance depends on the specific problem being solved. The packages includes the following optimizers: (a) 'adabelief' by Zhuang et al (2020), <arXiv:2010.07468>; (b) 'adabound' by Luo et al.(2019), <arXiv:1902.09843>; (c) 'adamw' by Loshchilov & Hutter (2019), <arXiv:1711.05101>; (d) 'madgrad' by Defazio and Jelassi (2021), <arXiv:2101.11075>; (e) 'nadam' by Dozat (2019), <https://openreview.net/pdf/OM0jvwB8jIp57ZJjtNEZ.pdf>; (f) 'qhadam' by Ma and Yarats(2019), <arXiv:1810.06801>; (g) 'radam' by Liu et al. (2019), <arXiv:1908.03265>; (h) 'swats' by Shekar and Sochee (2018), <arXiv:1712.07628>; (i) 'yogi' by Zaheer et al.(2019), <https:://papers.nips.cc/paper/8186-adaptive-methods-for-nonconvex-optimization>.
}
\seealso{
Useful links:
\itemize{
  \item \url{https://github.com/e-sensing/torchopt/}
}

}
\author{
\strong{Maintainer}: Gilberto Camara \email{gilberto.camara@inpe.br}

Authors:
\itemize{
  \item Rolf Simoes \email{rolf.simoes@inpe.br}
  \item Daniel Falbel \email{daniel.falbel@gmail.com}
  \item Felipe Souza \email{felipe.carvalho@inpe.br}
  \item Alber Sanchez \email{alber.ipia@inpe.br}
}

}
\keyword{internal}
