% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/adahessian.R
\name{optim_adahessian}
\alias{optim_adahessian}
\title{Adahessian optimizer}
\usage{
optim_adahessian(
  params,
  lr = 0.001,
  betas = c(0.9, 0.999),
  eps = 1e-08,
  weight_decay = 0,
  hessian_power = 0.5
)
}
\arguments{
\item{params}{Iterable of parameters to optimize.}

\item{lr}{Learning rate (default: 0.15).}

\item{betas}{Coefficients for computing
running averages of gradient
and is square(default: (0.9, 0.999)).}

\item{eps}{Term added to the denominator to improve
numerical stability (default: 1e-4).}

\item{weight_decay}{L2 penalty (default: 0).}

\item{hessian_power}{Hessian power (default: 1.0).}
}
\value{
An optimizer object implementing the \code{step} and \code{zero_grad} methods.
}
\description{
R implementation of the Adahessian optimizer proposed
by Yao et al.(2020). The original implementation is available at
https://github.com/jettify/pytorch-optimizer/blob/master/torch_optimizer/adahessian.py
}
\examples{
if (torch::torch_is_installed()) {
# function to demonstrate optimization
beale <- function(x, y) {
    log((1.5 - x + x * y)^2 + (2.25 - x - x * y^2)^2 + (2.625 - x + x * y^3)^2)
 }
# define optimizer
optim <- optim_adahessian
# define hyperparams
opt_hparams <- list(lr = 0.01)

# starting point
x0 <- 3
y0 <- 3
# create tensor
x <- torch::torch_tensor(x0, requires_grad = TRUE)
y <- torch::torch_tensor(y0, requires_grad = TRUE)
# instantiate optimizer
optim <- do.call(optim, c(list(params = list(x, y)), opt_hparams))
# run optimizer
steps <- 400
x_steps <- numeric(steps)
y_steps <- numeric(steps)
for (i in seq_len(steps)) {
    x_steps[i] <- as.numeric(x)
    y_steps[i] <- as.numeric(y)
    optim$zero_grad()
    z <- beale(x, y)
    z$backward(retain_graph = TRUE, create_graph = TRUE)
    optim$step()
}
print(paste0("starting value = ", beale(x0, y0)))
print(paste0("final value = ", beale(x_steps[steps], y_steps[steps])))
}
}
\references{
Yao, Z., Gholami, A., Shen, S., Mustafa, M., Keutzer, K., & Mahoney, M. (2021).
ADAHESSIAN: An Adaptive Second Order Optimizer for Machine Learning.
Proceedings of the AAAI Conference on Artificial Intelligence, 35(12), 10665-10673.
https://arxiv.org/abs/2006.00719
}
\author{
Rolf Simoes, \email{rolf.simoes@inpe.br}

Felipe Souza, \email{lipecaso@gmail.com}

Alber Sanchez, \email{alber.ipia@inpe.br}

Gilberto Camara, \email{gilberto.camara@inpe.br}
}
